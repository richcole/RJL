=== Methods and Arguments

In Ruby all slots are accessed via methods from the outside. This is
different to Java in which direct access to an objects fields from
outside is allowed. It is important because a desirable refactoring is
to replace a direct field access with a method that calculated the
value of the field. 

The syntax then for accessing a field cannot be different from that of
calling a getter method. Similarly for setter methods. In Ruby this is
provided by only having setter and getter methods, however once still
gets into confusion with derived types. Should derived implementations
be allowed to access fields in the parent?

That question bring up the notion of facets as used in Eiffel in which
an object identifies several different contracts that it has with
different types of other objects. So it might have one contract with
derived types and another contract with users of the object.

Coming back to the notion of calling methods that may have zero or
more arguments.

  x y z

The evaluation of this expression depends on the type returned when
looking up x. If x returns a function taking two arguments then y and
z will be consumed by that function. 

The problem with this approach is that it doesn't let you easily
return a function from a method because that function will be
evaluated at the return site. If the function has a side effect then
the number of evaluations of the function matter.

If we introduce syntax to denote the evaluation of a function then we
cannot admit direct access to object fields, rather we must force the
return on a function which is evaluated.

Lets consider an object having a method defined which returns a
function.

  ex = { x: 0 ; incr: { { self x = self x + 1 } } ;

In this case the intention is to return a function. How then can one
evaluate the function that was returned?

  ex incr ;

This will leave a function on the stack which is subsequently
ignored. Once can evaluate it by sending the call method:

  ex incr call ;

Once needs to be aware that assignment to a field of a method will
cause the method to be evaluated, but that returning a function from
function will not cause the function to be evaluated.

An aspect of this syntax not discussed yet is that it can be difficult
to read if the quantities are not known. Perhaps this is already
implicit in the examples so far, but consider again

  x y z ;

To interpret the meaning of this expression one must know what x, y
and z are. One gets no syntactic help. The alternative is to admit
brackets or named arguments so either:

  x name: y address: z ;

or 

  x(y, z)

The issue with named arguments is that one often wants to provide a
somewhat random selection of them

  Rect new { left = 10 ; width = 20 ; height = 100 } ;

The builder pattern is one way around this in languages which don't
support named arguments

  Rect builder left 10 width 20 height 100 create ;

It feels more natural to coflate objects and functions. So that

  { left = 10 ; width = 20 ; height = 100 }

Is a function that evaluates to an object with the slot assignments
given. That means the functions returning an element must have an
explcit return statement.

  { left = 10 ; width = 20; height = 100 ; 
    return (width * height - left) 
  }



=== Data Objects

It seems that quite a lot of programming is concerned with data
objects. Data objects are composed mostly of data and have few
operations, except perhaps some usual suspects like clone, or
serialize.

How should once introduce such data objects? Thinking of C style
structures one, but in RJL syntax one has

  type Point = { Number | return struct [
    x = Number ;
    y = Number ;
  ]};
  type IntegerPoint = Point Integer ;

This feels somewhat cumbersome in terms of syntax, particularly the
return statement. Having implicit returns from blocks of the last
statement evaluated means that the return statement is not required,
but also that blocks not returning an object need explicit syntax

  example = { 
    1 .. 10 each { i | print ("Number " + i) }
    nil
  }

Perhaps a special syntax can be introduced that allows one to indicate
a block returns nil, e.g.

  example = { 
    1 .. 10 each { i | print ("Number " + i) <}
  <}

In this case the <} indicates the nil return from the block. The above
example is also cumbersome in that "Point Integer" does not sound like
english, rather the adjective go before the function as in an Integer
Point rather than a Point Integer, one might also say "Point of
Integers", but particularly the pluralization is difficult to achieve.

=== Merging of Functions and Objects

This entry is mostly a rumination rather than a conclusion. Two
problems are discussed:

1. objects look a lot like functions of a single argument
2. getter functions in a good language should be indistinguishable
in client code from direct access to the slot

We note that Ruby has this second property while javascript does
not. Ruby achieves it in a similar fashion to newspeak, that is slots
are not accessible except within getters and so an object must have a
getter to make a slot accessible. 

An object can be regarded as a function that takes a symbol and
returns a value which may possibly be a function.

 type Object = function Symbol returns Object

So what then is a function, well a function? Well a function takes
zero or more objects passed to it and returns an object. It does this
using the execution stack.

There are however implicit evaluations in RJL which cause functions
returned by objects to be evaluated. Evaluation continues in RJL until
an object is returned that is not itself a function.

Another rule is that assignment of function literals does not involve
evaluation. So for example one can write:

  a = { return { x : String | sys print x } } ;

And then call

  a "Hello World" ;

This is evaluated by pushing "Hello World" onto the stack, then
pushing a. a will then be resolved which will yield a function, since
the top of the stack is a function it will be evaluated.  That
function will return another function which in turn will be evaluated
yielding a result.

This property prevents higher order functions from being written
because the function being returned will be evaluated. Consider

  b = a ;

We would like b to be assigned to the function returned from a, but
the closure returned will be evaluated. The reason that functions
returned as objects are evaluated is so that getters are
indistinguishable in clients from values stored in slots.

What if only a single evaluation is permitted of a function, that is
whenever a function is returned it is evaluated, but if that function
returns a function the result is not evaluated.

So then one has something like:

  b = a ;

becomes

  resolve 'a
  if ((stack top) is-a function) {
    eval [| (stack pop);  stack |]
  }
  perform_assignment 'b

So that if a block returns a function then the function returned is
not evaluated.

Under this definition then is an object still like a function? Well no
because the object returns a function, and that function would not be
evaluated. Rather if an object is involved then there are up to two
evaluations, the first returns the block to be executed and the second
executes the block.

Once can reduce the number of evaluations by using a ^ character. So
for example:

  print = sys ^print

This will not evaluate the block returned by looking up print in the
sys object.

What about chains of objects?

  a = b c d 

What if b returns a function taking a single argument and when given c
that again returns a function taking an argument d ? Then a kind of
implicit triple evaluation is required. The problem comes when one
considers blocks with no arguments, for these one cannot decide
when to stop evaluation by checking for an empty stack. Here the rule
is that an object lookup evaluates a method with zero arguments once
and no more.

Thus:

  x = { return { x | sys print x } }
  x "Hello World"

Yields a lookup of x returning a function of zero arguments duely
evaluated and returning a function of 1 argument which is evaluated by
virtue of an argument on the stack.

  x = { return { rand } }

Yields a lookup of x returning a function of zero arguments duely
evaluated and returning a function of 0 argument which is not
evaluated by virtue of it being a function of 0 arguments.

Thus this second example results in the assignment of a function to x
rather than a variable.

=== Setters and Lexical Closure

Consider the following 

  { i = 0 ;
    set_i = { i = 1 } ;
    set_i ;
  }

Should this set i in the outer scope to be 1 or not? My first
intuition is that it should set i in the outer scope to admit the
following function.

  { xs |
    sum = 0;
    xs each { x | sum += x };
    return sum;
  }

And what goes for 1 level of nesting goes for all levels of
nesting. Consider then also the following program:

  { set_i = { i = 1 };
    set_i;
  }

In the outer in this case the value of i should be nil because i was
not initialzied in an enclosing scope when the closure was invoked.

Similarly

  { set_i = { i = 1 } ;
    i = 0;
    set_i;
  }

Should set the variable i in the outer scope because i has been
defined by the time the closure is executed.

I have the feeling that this complexity of behaviour may be a bane to
the programmer who will have trouble keeping track of which variables
are in use and which are not. One might consider something like the
following for assignment:

  { set_i = { parent i = 1 };
    set_i ;
  }

Where parent simply points at the lexical closure. Here one must much
more deliberately make assignments within the enclosing scope, but one
must also count the levels as to assign two levels higher requires
doing "parent parent i = 1".

=== Support for Literate Programming

The idea of literate programming is that a program is encased in a
document which explains the program that is being outlined.

A single program however might have quite a number of documents
written about it which are intended for different audiences in
different contexts.

Lets consider however a syntax for literate programming that conforms
to the wiki language that I've been playing with.

* Any indented text is code
* All other code is wiki code

So for example the following is our canonical hello world example.

  { sys |
    sys print "Hello World\n"
  }

Under this style it is difficult to have the document nested inside
the code, but rather the code is nested inside the document.

Lets see how this can work for a function that we wish to document the
parts of. The first part of the program declares a block taking a
single argument, sys.

  { sys |
    
Next we use the sys argument to print to standard output of the process.

    sys print "Hello World\n" ;

And finally we close the block. Blocks which do not contain a return
code return nil.

  }

It can be difficult to following the general indentiation style when
the code is part of the document. But perhaps editors which 

=== Design Directions

* How to specify names of modules
* How to type modules
* How to make use of type information within IDE's
* What should a module really look like?

=== On the Smalltalk Dream

Highly dynamic languages do not have static types and this makes it
difficult to program against their libraries for two reasons:

1. refactoring support is quite limited because it isn't clear who
calls which functions

2. method name completion is quite limited because it isn't clear
while a bit of code is being authored what the types are

I think that without these two properties programming becomes
significantly more difficult. One can work around these deficiencies
by

1. programming on live objects, that is author the program while it is
running

2. writing lots of examples and documentation for libraries

If I think about a number of significant java libraries such as
jodatime and httpclient, these libraries are quite difficult to use
without having at least an introductory manual explaining the basic
types and what their methods are.

If I think about using rails then similarly it is quite difficult to
use without having a book with several examples introducing the major
objects and the ways in which they are intended to interact.

A quite useful but rarely implemented object is a simulator object. A
simulator simulates access to an object including allowing one to
generate the know failure conditions of the object. It makes sense for
the simulator object to be crafted by the crafter or the original
object, but it is rarely done and so users of the original object are
forced to construct mock objects to test their code.

=== On the Naming of Things

One needs to be able to name libraries and software versions. One also
needs to be able to describe the types of objects and these type
definitions must also have versions as they can change over time.

Names are used to specify dependencies. But specifying dependencies
also includes notions of types and versions. Lets consider an example,
say we have 

  libwidget-java version 1.0.3.1 build 10123

and we would like to say that this library requires a package that
conforms to a type defined as

  liblogging-api-java version >1.0 

This extends notions that exist in Debian because it admits build
numbers. To supply satisfy the dependency we need to know which
packages are available, which of them provide liblogging-api-java ...,
and we need a mechanism to choose one of them.

A related concern is that often one has a bunch of modules and one
want to be able to wire them up in some configuration. Once the
configuration is defined it can be checked to see if the module
requires are satisfied by it.

In a world where modules are objects and thus have state, knowing
whether or not two modules share access to a third is important.

=== Executing in a Lexical Scope

Often it is useful to be able to execute some code within a lexical
scope. This allows one to define a context in which operators are
defined.

For example one might provide access to some variables in a lexical
scope, e.g.

  std = [ stdio = sys io ; net = sys net ; types = sys types ] ;

Then one can execute a block within the lexical scope of that object,
so for example:

  using std { std |
    stdio print "Hello World" 
  }

The block is executed in the lexical scope of the object passed in
rather than in the lexical scope in which it was defined.

=== Dependencies

There will need to be some way for a module to declare dependence on
another module. One mechanism is to have a module return an object.

  [
    depends = { sys |
      using (sys depends) [| depends |
        "std.stdio"; 
        "std.net"; 
        "std.ui" > "0.1.0";
      |]
    }
  ]

Such an expression is rather verbose, can we factor this out as a
function?

  sys define_dependencies = { block | using (sys depends) block }

then

  depends = { sys | 
    sys define_dependencies [| depends |
      "std.stdio"; 
      "std.net"; 
      "std.ui" > "0.1.0";
    |]
  }

This is marginally simpler. It has hidden the fact that the block will
be executed in a dynamic lexical scope.

Note here that we assume that within different lexical scope objects
such as String may have different definitions.

Lets try to describe the type of such objects. First lets define the
notion of a module:

  sys define_types { type |
    type module = [
      type module_dependency = object;
      type module = [
        depends = array module_dependency;
      ]
    ]
  }

Here we say that a module_dependency is an object. We would like types
to be extensible. So rather than use = we should use += and also
closures so that module definitions can be recursively evaluated.

  sys define_types { type |
    type module_dependency += { object };
    type depends += {[ depends = array module_dependency; ]}; 
  }

Array and maps are executed when they're defined a opposed to blocks
which are executed when references.

We also need that a += b is an alias for a = a + b and that nil + x =
x.

=== Typing 

It makes sense to have typing in a programming language. Probably the
typing should be optional. So far I only really can imagine typing
objects, one wonders if one can have some more expansive notion of
typing.

Typing is about specifying interfaces. One should be able to say
whether a given object conforms to a type or not.

Lets begin by trying to define the notion of a sequence:

  { type |
    type sequence = [ a |
      each = function ( 'f : function a ) -> { sequence a }
        description ( 
          "Applies the function 'f to each element of the " +
          "sequence discarding the result and returning the " +
          "sequence"
        )
      ;
    ];
    type array = [ a |
      at  = function integer -> a ;
      set = function [| integer; a |] -> a ;
    ];
  }

We would execute such a type definition block within a lexical scope
which defines type.

In dynamic languages these types may or may not apply to any given
array or set. One should note that array is a parametric type taking a
single argument in the above.

Types are defined by executing the program. Type definitions are
themselves programs and are executed to produce type definitions.

Type expressions can be attached to any variable so for example via
the colon, so for example:

  { sys : 
      [ 
        stdio += [ print += function string -> object ] 
      ]
    |
    sys stdio print "Hello World";
  }

The types here must also be passed into the fragment being executed
probably from the lexical scope.

The programming environment then needs access to the lexical scope in
order to exploit the type information to perform type analysis on the
program.

Another way that the one can write the program is to write it as it is
being run. This encourages test driven development in that before one
can write an object one must have to hand the arguments that will be
passed into the object.

So far fragments have been defined with reference to a sys object. One
can however assume that the sys object is also in the lexical scope.

 { sys |
   using sys { sys |
     ...
   }
 }

Then one has access to the lexical closure in which a program is being
run.

=== Describing Interfaces

The approach to defining types is to construct them via a code sample
that constructs the types as objects as described in [[Typing]]. One
needs to be able to talk about types for interfaces when writing
systems because this is the only mechanism that exists for information
hiding.

One might have a system that affords tools that make it easier to
reason about the types that exist within a given programming system.

One might also like to go beyond what can be described by types when
talking about interfaces so for example give equations that are true
of an interface.

Why then use a formal language to describe interfaces. Because the
type definitions can then be processed by IDE's to facilitate checking
that programs conform to the type definition.



